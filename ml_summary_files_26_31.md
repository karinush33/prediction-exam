# סיכום מודלים (קבצים 26–31)

---

## קובץ 26: Polynomial Regression (רגרסיה פולינומיאלית)

- **שאלה 1: האם המודל פותר בעיית קלסיפיקציה, רגרסיה או גם וגם?**  
  - **תשובה:** רגרסיה.  
  - **הסבר:** המודל חוזה ערך מספרי רציף. אם אנחנו מנסים לחזות מחיר, גובה או כמות – זו תמיד רגרסיה.

- **שאלה 2: מה ההבדל המהותי בין רגרסיה לינארית לרגרסיה פולינומיאלית מבחינת הקשר בין X ל-Y?**  
  - **תשובה:** לינארית מחפשת קו ישר, פולינומיאלית מחפשת עקומה.  
  - **הסבר:** ברגרסיה לינארית הקשר הוא "קבוע" (על כל 1 ב־$X$ ה־$Y$ עולה ב־$a$). בפולינומיאלית הקשר משתנה – הוא יכול לעלות לאט ואז מהר, או לעלות ואז לרדת.

- **שאלה 3: מהו הרכיב המתמטי שהמודל מחפש ברגרסיה פולינומיאלית?**  
  - **תשובה:** מקדמים למשתנים בחזקות שונות ($X^2, X^3$ וכו').  
  - **הסבר:** המודל בונה משוואה מורכבת שבה הפיצ'ר מופיע גם בריבוע או בשלישית, מה שיוצר את הקימורים בגרף.

- **שאלה 4: מה המשמעות של דרגת הפולינום (degree) במודל?**  
  - **תשובה:** הדרגה קובעת את רמת המורכבות והגמישות של העקומה.  
  - **הסבר:** ככל שהדרגה גבוהה יותר, לעקומה יש יותר "פיתולים" והיא יכולה להיצמד יותר לנקודות הנתונים.

- **שאלה 5: האם דרגת הפולינום נלמדת מהנתונים או נקבעת מראש?**  
  - **תשובה:** נקבעת מראש על ידי המפתח.  
  - **הסבר:** המחשב לא יודע לבד איזו דרגה תתאים. אנחנו חייבים להחליט עליה לפני שמתחילים את תהליך האימון.

- **שאלה 6: האם דרגת הפולינום נחשבת Hyperparameter? הסבר/י מדוע.**  
  - **תשובה:** כן.  
  - **הסבר:** כל פרמטר שקובע את מבנה המודל ומוגדר לפני האימון (ולא נלמד תוך כדי) נקרא Hyperparameter.

- **שאלה 7: כיצד שינוי דרגת הפולינום משפיע על התאמת המודל לנתונים?**  
  - **תשובה:** דרגה גבוהה יותר גורמת למודל להיצמד יותר לנתונים.  
  - **הסבר:** זה מאפשר למודל לתאר קשרים מאוד מסובכים, אבל יש לזה מחיר של סיכון לטעות בנתונים חדשים.

- **שאלה 8: כיצד שינוי דרגת הפולינום משפיע על הסיכון ל-Overfitting ול-Underfitting?**  
  - **תשובה:** דרגה נמוכה מדי גורמת ל-Underfitting, דרגה גבוהה מדי גורמת ל-Overfitting.  
  - **הסבר:** $Degree=1$ (קו ישר) על נתונים עקומים יפספס את הדפוס (Under). דרגה גבוהה מדי (למשל $Degree=15$) תלמד כל רעש אקראי ותיצמד אליו במקום ללמוד את המגמה הכללית (Over).

- **שאלה 9: האם ניתן לפתור רגרסיה פולינומיאלית באמצעות Brute Force? אם כן – תאר/י את הרעיון הכללי.**  
  - **תשובה:** כן.  
  - **הסבר:** המחשב יכול לנסות "בכוח" המון שילובים של מקדמים לכל החזקות ($X^1, X^2, X^3$), לחשב $MSE$ לכל שילוב, ולבחור את אלו שנתנו את השגיאה הכי נמוכה.

- **שאלה 10: אילו מדדי ביצוע מתאימים להערכת מודל רגרסיה פולינומיאלית?**  
  - **תשובה:** $MSE, MAE, R^2, Adjusted\ R^2$.  
  - **הסבר:** אלו המדדים הסטנדרטיים לכל בעיית רגרסיה.

- **שאלה 11: האם ניתן להשתמש ברגרסיה פולינומיאלית עם יותר מפיצ'ר אחד? אם כן – כיצד זה משפיע על המודל?**  
  - **תשובה:** כן.  
  - **הסבר:** זה יוצר אינטראקציות בין פיצ'רים (למשל $X_1 \times X_2$). המודל נהיה הרבה יותר מורכב ומספר הפיצ'רים בטבלה גדל משמעותית.

- **שאלה 12: אם מבצעים רגרסיה פולינומיאלית עם פיצ'ר אחד ובוחרים דרגה שנייה, כמה פיצ'רים יופיעו בפועל בטבלת הנתונים לאחר הטרנספורמציה?**  
  - **תשובה:** 3 פיצ'רים.  
  - **הסבר:** $X^0$ (האיבר הקבוע/חיתוך), $X^1$ (הפיצ'ר המקורי), ו־$X^2$ (הפיצ'ר בריבוע).

---

## קובץ 27: Logistic Regression (רגרסיה לוגיסטית)

- **שאלה 1: האם המודל פותר בעיית קלסיפיקציה, רגרסיה או גם וגם?**  
  - **תשובה:** קלסיפיקציה (סיווג).  
  - **הסבר:** למרות השם "רגרסיה", המודל משמש לסיווג לקטגוריות (למשל: חולה/בריא).

- **שאלה 2: מהו סוג גרף הפלט של מודל רגרסיה לוגיסטית?**  
  - **תשובה:** פונקציית סיגמואיד (Sigmoid) בצורת S.  
  - **הסבר:** הגרף חסום בין 0 ל-1, מה שמאפשר להפוך כל מספר לערך של הסתברות.

- **שאלה 3: מה המשמעות של הערך שמחזיר המודל לפני קבלת החלטת הסיווג?**  
  - **תשובה:** הסתברות ($Probability$).  
  - **הסבר:** המודל מחזיר מספר בין 0 ל-1 המייצג את הסיכוי שהדוגמה שייכת למחלקה מסוימת.

- **שאלה 4: איזו פונקציה מתמטית מרכזית משמשת ברגרסיה לוגיסטית?**  
  - **תשובה:** פונקציית הסיגמואיד.  
  - **הסבר:** המשוואה היא: $\frac{1}{1 + e^{-x}}$.

- **שאלה 5: כיצד מתקבלת ההחלטה הסופית לאיזו מחלקה הדוגמה שייכת?**  
  - **תשובה:** על ידי השוואת ההסתברות לסף ההחלטה ($Threshold$).  
  - **הסבר:** אם ההסתברות שיצאה מהסיגמואיד גבוהה מהסף, נחליט שמדובר במחלקה 1.

- **שאלה 6: מהו Threshold ומה תפקידו במודל?**  
  - **תשובה:** סף ההחלטה (בדרך כלל 0.5).  
  - **הסבר:** הוא קובע איפה אנחנו מותחים את הקו בין "כן" ל"לא".

- **שאלה 7: מהו Decision Boundary במודל רגרסיה לוגיסטית?**  
  - **תשובה:** הגבול הגיאומטרי שמפריד בין המחלקות במרחב.  
  - **הסבר:** בדו-ממד זה יהיה קו ישר שמפריד בין הנקודות של סוג א' לנקודות של סוג ב'.

- **שאלה 8: כיצד Python פותר את חישוב המודל – נוסחה סגורה או קירוב מתמטי? הסבר/י.**  
  - **תשובה:** קירוב מתמטי (Optimization).  
  - **הסבר:** בניגוד לרגרסיה לינארית, כאן אין נוסחה אחת שפותרת הכל בבת אחת. המחשב משתמש בתהליך של ניסוי וטעייה חכם (כמו $Gradient\ Descent$) כדי להתקרב לפתרון האופטימלי.

- **שאלה 9: תאר כיצד אפשר באמצעות Brute Force למצוא נוסחא שעונה נכון על המדגם?**  
  - **תשובה:** ניסוי של המון שילובי משקלים.  
  - **הסבר:** המחשב מנסה מיליוני קווים מפרידים שונים עד שהוא מוצא את הקו שמפריד הכי טוב בין הקבוצות במדגם שנתנו לו.

- **שאלה 10: האם Threshold נחשב Hyperparameter? הסבר/י.**  
  - **תשובה:** כן.  
  - **הסבר:** המחשב לא לומד את הסף לבד. אנחנו קובעים אותו. אם נרצה להיות מחמירים יותר (למשל בזיהוי מחלות), נרים את הסף ל-0.8.

- **שאלה 11: מה ההבדל העקרוני בין רגרסיה לינארית לרגרסיה לוגיסטית, למרות השם הדומה?**  
  - **תשובה:** לינארית חוזה מספר רציף, לוגיסטית חוזה סיווג (קטגוריה).  
  - **הסבר:** לינארית מוציאה קו ישר שיכול להגיע לאינסוף. לוגיסטית מוציאה הסתברות שתמיד כלואה בין 0 ל-1.

- **שאלה 12: אילו מדדי ביצוע מתאימים להערכת מודל רגרסיה לוגיסטית?**  
  - **תשובה:** $Accuracy, Precision, Recall, F1\text{-}Score$.  
  - **הסבר:** אלו מדדים שבודקים הצלחה בסיווג (כמה צדקנו ב"כן" וב"לא").

- **שאלה 13: מהי מטריצת בלבול (Confusion Matrix) ומה ניתן ללמוד ממנה?**  
  - **תשובה:** טבלה המציגה את פירוט השגיאות וההצלחות של המודל.  
  - **הסבר:** היא מראה כמה פעמים המודל אמר "חיובי" וצדק, כמה פעמים טעה, וכנ"ל לגבי "שלילי". זה עוזר להבין אם המודל נוטה לטעות בסוג מסוים של סיווג.

- **שאלה 14: האם ניתן להשתמש ברגרסיה לוגיסטית עם יותר מפיצ'ר אחד? אם כן – כיצד זה משפיע על גבול ההחלטה?**  
  - **תשובה:** כן.  
  - **הסבר:** ככל שיש יותר פיצ'רים, גבול ההחלטה הופך מקו למישור או להיפר-מישור במרחב רב-ממדי.

---

## קובץ 28: KNN (השכנים הקרובים ביותר)

- **שאלה 1: האם האלגוריתם KNN פותר בעיית קלסיפיקציה, רגרסיה או גם וגם?**  
  - **תשובה:** גם וגם.  
  - **הסבר:** ניתן להשתמש בו גם לחיזוי קטגוריה וגם לחיזוי ממוצע של ערכים.

- **שאלה 2: מה המשמעות של השם KNN?**  
  - **תשובה:** $K\text{-}Nearest\ Neighbors$ (K השכנים הקרובים ביותר).

- **שאלה 3: מהו הרעיון המרכזי שעליו מבוסס האלגוריתם KNN?**  
  - **תשובה:** דוגמאות דומות נמצאות קרוב אחת לשנייה במרחב הנתונים.  
  - **הסבר:** כדי לדעת מה הסיווג של נקודה חדשה, נסתכל על השכנים שלה ונחליט לפיהם.

- **שאלה 4: מהו התפקיד של הפרמטר K באלגוריתם?**  
  - **תשובה:** קובע כמה שכנים לבדוק לפני שמקבלים החלטה.

- **שאלה 5: האם K נחשב Hyperparameter? הסבר/י.**  
  - **תשובה:** כן.  
  - **הסבר:** אנחנו קובעים אותו מראש. המודל לא לומד את ה-K האופטימלי לבד.

- **שאלה 6: כיצד בחירת ערך קטן של K משפיעה על המודל?**  
  - **תשובה:** המודל הופך לרגיש מאוד לרעש ($Overfitting$).  
  - **הסבר:** שכן אחד יוצא דופן (טעות בנתונים) יכול להטות את כל החיזוי.

- **שאלה 7: כיצד בחירת ערך גדול של K משפיעה על המודל?**  
  - **תשובה:** המודל הופך לכללי מדי ופחות מדויק ($Underfitting$).

- **שאלה 8: הסבר מה זה גרף ELBOW?**  
  - **תשובה:** גרף שעוזר למצוא את ה-K האידיאלי.  
  - **צירים:**  
    - ציר X = ערכי K השונים  
    - ציר Y = שיעור השגיאה (Error Rate)  
  - **Sweet Spot:** הנקודה ב"מרפק" של הגרף, שבה השגיאה יורדת משמעותית ומתחילה להתייצב.  
  - **דוגמה לעוד מודל:** מודל אשכולות $K\text{-}Means$, שבו משתמשים בגרף הזה כדי לקבוע את מספר הקבוצות האופטימלי.

- **שאלה 9: מה ההבדל בין KNN לקלסיפיקציה לבין KNN לרגרסיה?**  
  - **תשובה:** בשיטת קבלת ההחלטה.

- **שאלה 10: כיצד מתקבלת התחזית ב-KNN לקלסיפיקציה?**  
  - **תשובה:** לפי הרוב ($Majority\ Vote$).  
  - **הסבר:** אם מתוך 5 שכנים, 3 הם "חתול", החיזוי יהיה "חתול".

- **שאלה 11: כיצד מתקבלת התחזית ב-KNN לרגרסיה?**  
  - **תשובה:** לפי ממוצע השכנים.  
  - **הסבר:** אם השכנים הם בתים במחירים שונים, נחזה את הממוצע שלהם.

- **שאלה 12: איזו מדידת מרחק נפוצה משמשת ב-KNN?**  
  - **תשובה:** מרחק אוקלידי (Euclidean Distance).  
  - **הסבר:** חישוב "קו ישר" בין שתי נקודות.

- **שאלה 13: כיצד מספר הפיצ'רים משפיע על ביצועי KNN?**  
  - **תשובה:** יותר מדי פיצ'רים פוגעים בביצועים ("קללת הממדיות").  
  - **הסבר:** במרחב עם המון ממדים, כל הנקודות נראות רחוקות אחת מהשנייה והמרחק מאבד משמעות.

- **שאלה 14: האם קיים שלב אימון (Training) מובהק באלגוריתם KNN?**  
  - **תשובה:** לא.  
  - **הסבר:** האלגוריתם נקרא "לומד עצלן" ($Lazy\ Learner$). הוא פשוט שומר את כל הנתונים בזיכרון.

- **שאלה 15: כיצד Python פותר את חישוב KNN – נוסחה סגורה או חישוב ישיר בזמן החיזוי?**  
  - **תשובה:** חישוב ישיר בזמן החיזוי.  
  - **הסבר:** רק כשמבקשים חיזוי, המחשב מתחיל למדוד מרחקים בין הנקודה החדשה לכל שאר הנקודות.

- **שאלה 16: מהם היתרונות המרכזיים של KNN?**  
  - **תשובה:** פשטות, אין צורך בסידור מוקדם של הנתונים, גמיש מאוד.

- **שאלה 17: מהם החסרונות המרכזיים של KNN?**  
  - **תשובה:** איטי מאוד בחיזוי על דאטה גדול, דורש המון זיכרון, רגיש לפיצ'רים לא רלוונטיים.

---

## קובץ 29: SVM (Support Vector Machines)

- **שאלה 1: האם המודל SVM פותר בעיית קלסיפיקציה, רגרסיה או גם וגם?**  
  - **תשובה:** גם וגם (בדרך כלל משמש לקלסיפיקציה).

- **שאלה 2: מהו הרעיון המרכזי שעליו מבוסס מודל SVM?**  
  - **תשובה:** מציאת המפריד (Hyperplane) שמייצר את המרווח ($Margin$) המקסימלי בין שתי המחלקות.

- **שאלה 3: מהו Margin במודל SVM? האם שני ה-Margins צריכים להיות במרחק שווה?**  
  - **תשובה:** המרווח הוא המרחק בין קו ההפרדה לנקודות הקרובות ביותר. כן, הם צריכים להיות שווים.  
  - **הסבר:** המודל מחפש את ה"נתיב" הכי רחב שאפשר להעביר בין שתי הקבוצות.

- **שאלה 4: מהם Support Vectors?**  
  - **תשובה:** אלו הן נקודות הנתונים שנמצאות ממש על גבול המרווח.  
  - **הסבר:** הן הנקודות ש"תומכות" בקו ההפרדה. אם נזיז אותן, כל המודל ישתנה. שאר הנקודות לא מעניינות את המודל.

- **שאלה 5: האם אפשר לפתור את בעיית SVM באמצעות Brute Force?**  
  - **תשובה:** כן.  
  - **הסבר:** המחשב יכול לנסות אינסוף קווים מפרידים ולבדוק עבור כל אחד מהו המרווח שהוא יוצר, עד שימצא את הקו עם המרווח הכי רחב.

- **שאלה 6: האם SVM משתמש בפתרון בנוסחה סגורה או בקירוב מתמטי? הסבר/י.**  
  - **תשובה:** קירוב מתמטי (Optimization).  
  - **הסבר:** מדובר בבעיית אופטימיזציה מורכבת שדורשת חישובים חוזרים עד למציאת הפתרון הטוב ביותר.

- **שאלה 7: מהם היתרונות המרכזיים של SVM?**  
  - **תשובה:** חזק מאוד בממדים גבוהים (הרבה פיצ'רים), יעיל כשיש הפרדה ברורה בין קבוצות.

- **שאלה 8: מהם החסרונות המרכזיים של SVM?**  
  - **תשובה:** לא מתאים למאגרי נתונים עצומים (איטי), רגיש מאוד לרעש (נקודות שחודרות לצד השני).

- **שאלה 9: הסבר את תהליך ה-Train/Test Split ומדוע כדאי לעבוד בתהליך זה.**  
  - **תשובה:** חלוקת הנתונים ל-80% אימון ו-20% בדיקה.  
  - **הסבר:** אנחנו מאמנים את המודל על חלק מהנתונים ובודקים אותו על נתונים שהוא "מעולם לא ראה". זו הדרך היחידה לדעת אם המודל באמת למד או רק זכר את התשובות בעל פה.

- **שאלה 10: אילו מדדי ביצוע מתאימים להערכת מודל SVM בקלסיפיקציה?**  
  - **תשובה:** $Accuracy, Precision, Recall, F1\text{-}Score$.

---

## קובץ 30: Decision Tree (עץ החלטה)

- **שאלה 1: האם מודל Decision Tree פותר בעיית קלסיפיקציה, רגרסיה או גם וגם?**  
  - **תשובה:** גם וגם.

- **שאלה 2: מהו הרעיון המרכזי שעליו מבוסס מודל Decision Tree?**  
  - **תשובה:** פיצול הנתונים לסדרת שאלות (עץ) עד שמגיעים להחלטה סופית.

- **שאלה 3: מהו Gini Impurity?**  
  - **תשובה:** מדד למידת ה"ערבוב" של המחלקות בתוך צומת בעץ.

- **שאלה 4: כיצד Gini Impurity מודד את איכות הפיצול?**  
  - **תשובה:** ככל שהערך נמוך יותר, הפיצול טוב יותר.  
  - **הסבר:** אנחנו רוצים שבכל עלה של העץ יהיו רק דוגמאות מסוג אחד (למשל רק "חולים").

- **שאלה 5: מה הערך של Gini Impurity כאשר הצומת "טהור" לחלוטין?**  
  - **תשובה:** 0.

- **שאלה 6: מהי מטרת הפיצול בכל צומת בעץ?**  
  - **תשובה:** להקטין את ה-$Gini$ (להגדיל את ה"טוהר" של הקבוצות).

- **שאלה 7: הסבר את תצורת הפיצול בעץ כאשר מדובר בבעיית רגרסיה.**  
  - **תשובה:** הפיצול נעשה כדי לצמצם את השונות ($Variance$) בתוך הקבוצות.

- **שאלה 8: כאשר הגענו לתחתית העץ כיצד מחושב ערך התחזית מ-2 העלים?**  
  - **תשובה:** לפי הממוצע של הדגימות שנמצאות באותו עלה.

- **שאלה 9: אילו Hyperparameters נפוצים קיימים ב-Decision Tree?**  
  - **תשובה:** עומק מקסימלי ($max\_depth$), מינימום דגימות לפיצול ($min\_samples\_split$).

- **שאלה 10: כיצד עומק העץ משפיע על Overfitting ו-Underfitting?**  
  - **תשובה:** עץ עמוק מאוד = $Overfitting$. עץ רדוד מאוד = $Underfitting$.  
  - **הסבר:** עץ עמוק מדי לומד כל מקרה ספציפי (כולל טעויות). עץ רדוד מדי פשוט מדי ולא לומד את המורכבות.

- **שאלה 11: אילו מדדי ביצוע מתאימים להערכת מודל Decision Tree?**  
  - **תשובה:**  
    - קלסיפיקציה: $Accuracy$  
    - רגרסיה: $MSE, R^2$

---

## קובץ 31: Random Forest (יער אקראי)

- **שאלה 1: האם מודל Random Forest פותר בעיית קלסיפיקציה, רגרסיה או גם וגם?**  
  - **תשובה:** גם וגם.

- **שאלה 2: מהו הרעיון המרכזי ואיך הוא קשור ל-Decision Tree?**  
  - **תשובה:** בניית הרבה עצי החלטה שונים ושילוב התוצאות שלהם.  
  - **הסבר:** עץ אחד יכול לטעות, אבל ממוצע של 100 עצים יהיה הרבה יותר מדויק.

- **שאלה 3: מהו Bootstrap Sampling וכיצד הוא משמש ב-Random Forest?**  
  - **תשובה:** דגימה אקראית עם החזרות מהנתונים המקוריים.  
  - **הסבר:** כל עץ ביער מתאמן על "דגימה" קצת שונה של נתונים, כדי שהעצים לא יהיו זהים.

- **שאלה 4: מדוע השימוש באקראיות משפר את ביצועי המודל?**  
  - **תשובה:** הוא מוריד את ה-$Variance$ (השונות) ומונע $Overfitting$.

- **שאלה 5: כיצד מתקבלת התחזית הסופית בקלסיפיקציה ומנגנון ההצבעה (Voting)?**  
  - **תשובה:** הרוב קובע.  
  - **הסבר:** אם 70 עצים אמרו "ספאם" ו-30 אמרו "לא ספאם", החלטת היער תהיה "ספאם".

- **שאלה 6: כיצד מתקבלת התחזית הסופית ברגרסיה?**  
  - **תשובה:** ממוצע של כל התחזיות של כל העצים.

- **שאלה 7: אילו Hyperparameters נפוצים קיימים במודל Random Forest?**  
  - **תשובה:** מספר העצים ביער ($n\_estimators$), עומק העצים, מספר הפיצ'רים שכל עץ בוחן.

- **שאלה 8: מהו OOB Error וכיצד משתמשים בו?**  
  - **תשובה:** $Out\text{-}Of\text{-}Bag\ Error$.  
  - **הסבר:** מכיוון שכל עץ לא ראה חלק מהנתונים (בגלל ה-$Bootstrap$), אפשר לבדוק את הביצועים שלו על הנתונים שהוא לא ראה. זה נותן הערכה טובה לדיוק המודל בלי צורך בסט בדיקה נפרד.

- **שאלה 9: מה היתרון של Random Forest מבחינת יציבות המודל לעומת Decision Tree?**  
  - **תשובה:** הוא הרבה יותר יציב ופחות רגיש לשינויים קטנים בנתונים.

- **שאלה 10: אילו מדדי ביצוע מתאימים להערכה?**  
  - **תשובה:**  
    - קלסיפיקציה: $Accuracy, Precision, Recall$  
    - רגרסיה: $MSE, R^2$
